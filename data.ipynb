{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["wJwJienFU1TJ"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-21T13:41:53.139357Z","iopub.execute_input":"2023-07-21T13:41:53.140122Z","iopub.status.idle":"2023-07-21T13:41:53.187285Z","shell.execute_reply.started":"2023-07-21T13:41:53.140089Z","shell.execute_reply":"2023-07-21T13:41:53.185053Z"},"trusted":true,"id":"ZzTYa3wmU1Rf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions\n","import contractions"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:41:58.624187Z","iopub.execute_input":"2023-07-21T13:41:58.624597Z","iopub.status.idle":"2023-07-21T13:42:11.942554Z","shell.execute_reply.started":"2023-07-21T13:41:58.624574Z","shell.execute_reply":"2023-07-21T13:42:11.940888Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"Lr-ACZReU1ST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install evaluate\n","import evaluate"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:42:17.287776Z","iopub.execute_input":"2023-07-21T13:42:17.288177Z","iopub.status.idle":"2023-07-21T13:42:47.061793Z","shell.execute_reply.started":"2023-07-21T13:42:17.288146Z","shell.execute_reply":"2023-07-21T13:42:47.059729Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"yQ0eKa78U1Sa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55lohDD4VveD","executionInfo":{"status":"ok","timestamp":1690041931154,"user_tz":-420,"elapsed":31560,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}},"outputId":"ccda108f-c9ca-48c5-9654-96461db2ad55"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["import re\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    roc_auc_score,\n","    roc_curve,\n","    auc,\n","    confusion_matrix,\n","    classification_report,\n",")\n","\n","from nltk.corpus import stopwords\n","import seaborn as sns"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:42:53.170339Z","iopub.execute_input":"2023-07-21T13:42:53.170746Z","iopub.status.idle":"2023-07-21T13:42:54.309645Z","shell.execute_reply.started":"2023-07-21T13:42:53.170717Z","shell.execute_reply":"2023-07-21T13:42:54.308835Z"},"trusted":true,"id":"0p82iAJgU1Sd","executionInfo":{"status":"ok","timestamp":1690042014193,"user_tz":-420,"elapsed":1445,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZZMJhKQXvv2","executionInfo":{"status":"ok","timestamp":1690042311488,"user_tz":-420,"elapsed":533,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}},"outputId":"9cf129d0-77b6-4796-d8a2-b68d1d06a07f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### Uploading train data"],"metadata":{"id":"RMSwOaGwU1Sg"}},{"cell_type":"code","source":["train_tw = pd.read_csv('/content/drive/MyDrive/AID/NLP/NLP-tweet/train.csv')\n","# train_tw = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:43:00.892742Z","iopub.execute_input":"2023-07-21T13:43:00.893153Z","iopub.status.idle":"2023-07-21T13:43:00.944638Z","shell.execute_reply.started":"2023-07-21T13:43:00.893123Z","shell.execute_reply":"2023-07-21T13:43:00.943741Z"},"trusted":true,"id":"Dljl1hQgU1Sk","executionInfo":{"status":"ok","timestamp":1690042083545,"user_tz":-420,"elapsed":1028,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Exploratory data analysis"],"metadata":{"id":"EfslrFUnU1So"}},{"cell_type":"markdown","source":["### Looking at the data"],"metadata":{"id":"O27shlsjU1St"}},{"cell_type":"code","source":["train_tw.head()"],"metadata":{"trusted":true,"id":"Dt70AlVlU1Sv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.keyword.value_counts().index.to_list()"],"metadata":{"trusted":true,"id":"iX1299usU1Sx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.info()"],"metadata":{"trusted":true,"id":"HIAOPHEBU1Sy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["locations = train_tw.location.value_counts().index.to_list()\n","print(locations)"],"metadata":{"trusted":true,"id":"pccVo8nUU1Sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.shape"],"metadata":{"trusted":true,"id":"tvsAMxVpU1S1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.target.value_counts()"],"metadata":{"trusted":true,"id":"CokXTU53U1S1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print (train_tw.loc[3107: 3142, 'text'])\n","for row in range(3107, 3142):\n","    print (train_tw.loc[row, 'text'])"],"metadata":{"trusted":true,"id":"hpVpNoYOU1S1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Let us now see the distribution of the tweets lengths in quantiles."],"metadata":{"id":"iA8ePvAWU1S4"}},{"cell_type":"code","source":["train_tw['tweet_length'] = [len(tweet.split()) for tweet in train_tw.text]\n","train_tw.head()"],"metadata":{"trusted":true,"id":"gjitBLFIU1S4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.tweet_length.value_counts()"],"metadata":{"trusted":true,"id":"tsmIJ3lcU1S7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.tweet_length.nunique()"],"metadata":{"trusted":true,"id":"bExVmAoxU1S8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_tw['tweet_length'].quantile([0.01, 0.05, 0.10, 0.5, 0.9, 0.95, 0.99]))"],"metadata":{"trusted":true,"id":"PfmRxegCU1S_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_tweets_length_distrib(column):\n","\n","    fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n","    fig = plt.figure(figsize=(5, 10))\n","\n","    lengths = [len(tweet.split()) for tweet in train_tw[column]]\n","    ax[0].hist(lengths, bins=31, linewidth=0.5, edgecolor=\"white\")\n","\n","    lengths = [len(tweet.split()) for tweet in train_tw[train_tw['target'] == 1][column]]\n","    ax[1].hist(lengths, bins=31, linewidth=0.5, edgecolor=\"white\")\n","\n","    lengths = [len(tweet.split()) for tweet in train_tw[train_tw['target'] == 0][column]]\n","    ax[2].hist(lengths, bins=31, linewidth=0.5, edgecolor=\"white\")\n","\n","    names = ['Common distribution', 'Distribution of disaster tweets.',\n","             'Distribution of calm tweets.']\n","    for i in range(len(names)):\n","        ax[i].set_title(names[i])\n","        ax[i].set_xlabel('Length')\n","        ax[i].set_ylabel('Number of tweets')"],"metadata":{"trusted":true,"id":"AmtaYr4KU1TA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_tweets_length_distrib('text')"],"metadata":{"trusted":true,"id":"2fudUXxKU1TB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Processing tweets. Let us try to find in the tweets characteristic symbols, like as URL's, html's, special characters, @ etc. In case some of them are found, we remove them."],"metadata":{"id":"wJwJienFU1TJ"}},{"cell_type":"code","source":["# We introduce counters for every matching found to see how many of them are found:\n","matches = {'URL': 0, 'html': 0, 'spec_char': 0, 'at': 0, 'hashtags': 0, 'non_ascii': 0,\n","           'multiples_!': 0, 'multiples_?': 0, 'stop words': 0,\n","           'typos': 0, 'acronyms': 0, 'abbr': 0}\n","\n","# Patterns for URL's\n","https_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n","# Patterns for html's\n","html_pattern = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n","# Patterns for special characters\n","emoji_pattern = re.compile(\n","                            '['\n","                            u'\\U0001F600-\\U0001F64F'  # emoticons\n","                            u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n","                            u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n","                            u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n","                            u'\\U00002702-\\U000027B0'\n","                            u'\\U000024C2-\\U0001F251'\n","                            ']+',\n","                            flags=re.UNICODE\n","    )\n","# Patterns for character sequences with At symbol (@)\n","at_pattern = re.compile(r'@\\S*')\n","# Patterns for hashtag symbols\n","hashtag_pattern = re.compile(r'#([^\\s]+)')\n","# Patterns for non_ascii characters:\n","non_ascii_pattern = re.compile(r'[^\\x00-\\x7f]')\n","# Patterns for sequences of several '!'\n","multiples_pattern_1 = re.compile(r'!+')\n","# Patterns for sequences of several '?'\n","multiples_pattern_2 = re.compile(r'\\?+')\n","\n","# Pattern for english stopwords\n","stopwords_list = stopwords.words('english')\n","stopwords_pattern = re.compile(r'(?<!\\w)(' + '|'.join(word \\\n","                        for word in stopwords_list) + r')(?!\\w)')\n","\n","# Taking into account typos, slang and other:\n","sample_typos_slang = {\n","                        \"w/e\": \"whatever\",\n","                        \"usagov\": \"usa government\",\n","                        \"recentlu\": \"recently\",\n","                        \"ph0tos\": \"photos\",\n","                        \"amirite\": \"am i right\",\n","                        \"exp0sed\": \"exposed\",\n","                        \"<3\": \"love\",\n","                        \"luv\": \"love\",\n","                        \"amageddon\": \"armageddon\",\n","                        \"trfc\": \"traffic\",\n","                        \"16yr\": \"16 year\"\n","                        }\n","sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) \\\n","                            for key in sample_typos_slang.keys()) + r')(?!\\w)')\n","\n","# Acronyms\n","sample_acronyms =  {\n","                    \"mh370\": \"malaysia airlines flight 370\",\n","                    \"okwx\": \"oklahoma city weather\",\n","                    \"arwx\": \"arkansas weather\",\n","                    \"gawx\": \"georgia weather\",\n","                    \"scwx\": \"south carolina weather\",\n","                    \"cawx\": \"california weather\",\n","                    \"tnwx\": \"tennessee weather\",\n","                    \"azwx\": \"arizona weather\",\n","                    \"alwx\": \"alabama weather\",\n","                    \"usnwsgov\": \"united states national weather service\",\n","                    \"2mw\": \"tomorrow\"\n","                    }\n","sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) \\\n","                            for key in sample_acronyms.keys()) + r')(?!\\w)')\n","\n","# Some common english abbreviations:\n","sample_abbr = {     \"$\" : \" dollar \",\n","                    \"€\" : \" euro \",\n","                    \"4ao\" : \"for adults only\",\n","                    \"a.m\" : \"before midday\",\n","                    \"a3\" : \"anytime anywhere anyplace\",\n","                    \"aamof\" : \"as a matter of fact\",\n","                    \"acct\" : \"account\",\n","                    \"adih\" : \"another day in hell\",\n","                    \"afaic\" : \"as far as i am concerned\",\n","                    \"afaict\" : \"as far as i can tell\",\n","                    \"afaik\" : \"as far as i know\",\n","                    \"afair\" : \"as far as i remember\",\n","                    \"afk\" : \"away from keyboard\",\n","                    \"app\" : \"application\",\n","                    \"approx\" : \"approximately\",\n","                    \"apps\" : \"applications\",\n","                    \"asap\" : \"as soon as possible\",\n","                    \"asl\" : \"age, sex, location\",\n","                    \"atk\" : \"at the keyboard\",\n","                    \"ave.\" : \"avenue\",\n","                    \"aymm\" : \"are you my mother\",\n","                    \"ayor\" : \"at your own risk\",\n","                    \"b&b\" : \"bed and breakfast\",\n","                    \"b+b\" : \"bed and breakfast\",\n","                    \"b.c\" : \"before christ\",\n","                    \"b2b\" : \"business to business\",\n","                    \"b2c\" : \"business to customer\",\n","                    \"b4\" : \"before\",\n","                    \"b4n\" : \"bye for now\",\n","                    \"b@u\" : \"back at you\",\n","                    \"bae\" : \"before anyone else\",\n","                    \"bak\" : \"back at keyboard\",\n","                    \"bbbg\" : \"bye bye be good\",\n","                    \"bbc\" : \"british broadcasting corporation\",\n","                    \"bbias\" : \"be back in a second\",\n","                    \"bbl\" : \"be back later\",\n","                    \"bbs\" : \"be back soon\",\n","                    \"be4\" : \"before\",\n","                    \"bfn\" : \"bye for now\",\n","                    \"blvd\" : \"boulevard\",\n","                    \"bout\" : \"about\",\n","                    \"brb\" : \"be right back\",\n","                    \"bros\" : \"brothers\",\n","                    \"brt\" : \"be right there\",\n","                    \"bsaaw\" : \"big smile and a wink\",\n","                    \"btw\" : \"by the way\",\n","                    \"bwl\" : \"bursting with laughter\",\n","                    \"c/o\" : \"care of\",\n","                    \"cet\" : \"central european time\",\n","                    \"cf\" : \"compare\",\n","                    \"cia\" : \"central intelligence agency\",\n","                    \"csl\" : \"can not stop laughing\",\n","                    \"cu\" : \"see you\",\n","                    \"cul8r\" : \"see you later\",\n","                    \"cv\" : \"curriculum vitae\",\n","                    \"cwot\" : \"complete waste of time\",\n","                    \"cya\" : \"see you\",\n","                    \"cyt\" : \"see you tomorrow\",\n","                    \"dae\" : \"does anyone else\",\n","                    \"dbmib\" : \"do not bother me i am busy\",\n","                    \"diy\" : \"do it yourself\",\n","                    \"dm\" : \"direct message\",\n","                    \"dwh\" : \"during work hours\",\n","                    \"e123\" : \"easy as one two three\",\n","                    \"eet\" : \"eastern european time\",\n","                    \"eg\" : \"example\",\n","                    \"embm\" : \"early morning business meeting\",\n","                    \"encl\" : \"enclosed\",\n","                    \"encl.\" : \"enclosed\",\n","                    \"etc\" : \"and so on\",\n","                    \"faq\" : \"frequently asked questions\",\n","                    \"fawc\" : \"for anyone who cares\",\n","                    \"fb\" : \"facebook\",\n","                    \"fc\" : \"fingers crossed\",\n","                    \"fig\" : \"figure\",\n","                    \"fimh\" : \"forever in my heart\",\n","                    \"ft.\" : \"feet\",\n","                    \"ft\" : \"featuring\",\n","                    \"ftl\" : \"for the loss\",\n","                    \"ftw\" : \"for the win\",\n","                    \"fwiw\" : \"for what it is worth\",\n","                    \"fyi\" : \"for your information\",\n","                    \"g9\" : \"genius\",\n","                    \"gahoy\" : \"get a hold of yourself\",\n","                    \"gal\" : \"get a life\",\n","                    \"gcse\" : \"general certificate of secondary education\",\n","                    \"gfn\" : \"gone for now\",\n","                    \"gg\" : \"good game\",\n","                    \"gl\" : \"good luck\",\n","                    \"glhf\" : \"good luck have fun\",\n","                    \"gmt\" : \"greenwich mean time\",\n","                    \"gmta\" : \"great minds think alike\",\n","                    \"gn\" : \"good night\",\n","                    \"g.o.a.t\" : \"greatest of all time\",\n","                    \"goat\" : \"greatest of all time\",\n","                    \"goi\" : \"get over it\",\n","                    \"gps\" : \"global positioning system\",\n","                    \"gr8\" : \"great\",\n","                    \"gratz\" : \"congratulations\",\n","                    \"gyal\" : \"girl\",\n","                    \"h&c\" : \"hot and cold\",\n","                    \"hp\" : \"horsepower\",\n","                    \"hr\" : \"hour\",\n","                    \"hrh\" : \"his royal highness\",\n","                    \"ht\" : \"height\",\n","                    \"ibrb\" : \"i will be right back\",\n","                    \"ic\" : \"i see\",\n","                    \"icq\" : \"i seek you\",\n","                    \"icymi\" : \"in case you missed it\",\n","                    \"idc\" : \"i do not care\",\n","                    \"idgadf\" : \"i do not give a damn fuck\",\n","                    \"idgaf\" : \"i do not give a fuck\",\n","                    \"idk\" : \"i do not know\",\n","                    \"ie\" : \"that is\",\n","                    \"i.e\" : \"that is\",\n","                    \"ifyp\" : \"i feel your pain\",\n","                    \"IG\" : \"instagram\",\n","                    \"iirc\" : \"if i remember correctly\",\n","                    \"ilu\" : \"i love you\",\n","                    \"ily\" : \"i love you\",\n","                    \"imho\" : \"in my humble opinion\",\n","                    \"imo\" : \"in my opinion\",\n","                    \"imu\" : \"i miss you\",\n","                    \"iow\" : \"in other words\",\n","                    \"irl\" : \"in real life\",\n","                    \"j4f\" : \"just for fun\",\n","                    \"jic\" : \"just in case\",\n","                    \"jk\" : \"just kidding\",\n","                    \"jsyk\" : \"just so you know\",\n","                    \"l8r\" : \"later\",\n","                    \"lb\" : \"pound\",\n","                    \"lbs\" : \"pounds\",\n","                    \"ldr\" : \"long distance relationship\",\n","                    \"lmao\" : \"laugh my ass off\",\n","                    \"lmfao\" : \"laugh my fucking ass off\",\n","                    \"lol\" : \"laughing out loud\",\n","                    \"ltd\" : \"limited\",\n","                    \"ltns\" : \"long time no see\",\n","                    \"m8\" : \"mate\",\n","                    \"mf\" : \"motherfucker\",\n","                    \"mfs\" : \"motherfuckers\",\n","                    \"mfw\" : \"my face when\",\n","                    \"mofo\" : \"motherfucker\",\n","                    \"mph\" : \"miles per hour\",\n","                    \"mr\" : \"mister\",\n","                    \"mrw\" : \"my reaction when\",\n","                    \"ms\" : \"miss\",\n","                    \"mte\" : \"my thoughts exactly\",\n","                    \"nagi\" : \"not a good idea\",\n","                    \"nbc\" : \"national broadcasting company\",\n","                    \"nbd\" : \"not big deal\",\n","                    \"nfs\" : \"not for sale\",\n","                    \"ngl\" : \"not going to lie\",\n","                    \"nhs\" : \"national health service\",\n","                    \"nrn\" : \"no reply necessary\",\n","                    \"nsfl\" : \"not safe for life\",\n","                    \"nsfw\" : \"not safe for work\",\n","                    \"nth\" : \"nice to have\",\n","                    \"nvr\" : \"never\",\n","                    \"nyc\" : \"new york city\",\n","                    \"oc\" : \"original content\",\n","                    \"og\" : \"original\",\n","                    \"ohp\" : \"overhead projector\",\n","                    \"oic\" : \"oh i see\",\n","                    \"omdb\" : \"over my dead body\",\n","                    \"omg\" : \"oh my god\",\n","                    \"omw\" : \"on my way\",\n","                    \"p.a\" : \"per annum\",\n","                    \"p.m\" : \"after midday\",\n","                    \"pm\" : \"prime minister\",\n","                    \"poc\" : \"people of color\",\n","                    \"pov\" : \"point of view\",\n","                    \"pp\" : \"pages\",\n","                    \"ppl\" : \"people\",\n","                    \"prw\" : \"parents are watching\",\n","                    \"ps\" : \"postscript\",\n","                    \"pt\" : \"point\",\n","                    \"ptb\" : \"please text back\",\n","                    \"pto\" : \"please turn over\",\n","                    \"qpsa\" : \"what happens\", #\"que pasa\",\n","                    \"ratchet\" : \"rude\",\n","                    \"rbtl\" : \"read between the lines\",\n","                    \"rlrt\" : \"real life retweet\",\n","                    \"rofl\" : \"rolling on the floor laughing\",\n","                    \"roflol\" : \"rolling on the floor laughing out loud\",\n","                    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","                    \"rt\" : \"retweet\",\n","                    \"ruok\" : \"are you ok\",\n","                    \"sfw\" : \"safe for work\",\n","                    \"sk8\" : \"skate\",\n","                    \"smh\" : \"shake my head\",\n","                    \"sq\" : \"square\",\n","                    \"srsly\" : \"seriously\",\n","                    \"ssdd\" : \"same stuff different day\",\n","                    \"tbh\" : \"to be honest\",\n","                    \"tbs\" : \"tablespooful\",\n","                    \"tbsp\" : \"tablespooful\",\n","                    \"tfw\" : \"that feeling when\",\n","                    \"thks\" : \"thank you\",\n","                    \"tho\" : \"though\",\n","                    \"thx\" : \"thank you\",\n","                    \"tia\" : \"thanks in advance\",\n","                    \"til\" : \"today i learned\",\n","                    \"tl;dr\" : \"too long i did not read\",\n","                    \"tldr\" : \"too long i did not read\",\n","                    \"tmb\" : \"tweet me back\",\n","                    \"tntl\" : \"trying not to laugh\",\n","                    \"ttyl\" : \"talk to you later\",\n","                    \"u.s.\" : 'usa',\n","                    \"u\" : \"you\",\n","                    \"u2\" : \"you too\",\n","                    \"u4e\" : \"yours for ever\",\n","                    \"utc\" : \"coordinated universal time\",\n","                    \"w/\" : \"with\",\n","                    \"w/o\" : \"without\",\n","                    \"w8\" : \"wait\",\n","                    \"wassup\" : \"what is up\",\n","                    \"wb\" : \"welcome back\",\n","                    \"wtf\" : \"what the fuck\",\n","                    \"wtg\" : \"way to go\",\n","                    \"wtpa\" : \"where the party at\",\n","                    \"wuf\" : \"where are you from\",\n","                    \"wuzup\" : \"what is up\",\n","                    \"wywh\" : \"wish you were here\",\n","                    \"yd\" : \"yard\",\n","                    \"ygtr\" : \"you got that right\",\n","                    \"ynk\" : \"you never know\",\n","                    \"zzz\" : \"sleeping bored and tired\"\n","                    }\n","sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) \\\n","                            for key in sample_abbr.keys()) + r')(?!\\w)')\n","\n","def find_all_URL(text):\n","    match_obj = re.findall(https_pattern, text)\n","    matches['URL'] += len(match_obj)\n","\n","def remove_URL(text):\n","    # Remove URL's from the string text:\n","    text = re.sub(https_pattern, \"\", text)\n","    return text\n","\n","def find_all_html(text):\n","    match_obj = re.findall(html_pattern, text)\n","    matches['html'] += len(match_obj)\n","\n","def remove_html(text):\n","    # Remove html from the string text:\n","    text = re.sub(html_pattern, \"\", text)\n","    return text\n","\n","def find_all_spec_char(text):\n","    match_obj = re.findall(emoji_pattern, text)\n","    matches['spec_char'] += len(match_obj)\n","\n","def remove_spec_char(text):\n","    # Remove special characters (like emojis and different graphic characters):\n","    text = emoji_pattern.sub(r'', text)\n","    return text\n","\n","def find_all_at(text):\n","    match_obj = at_pattern.findall(text)\n","    matches['at'] += len(match_obj)\n","\n","def remove_at(text):\n","    text = at_pattern.sub(r'', text)\n","    return text\n","\n","def find_all_hashtags(text):\n","    match_obj = hashtag_pattern.findall(text)\n","    matches['hashtags'] += len(match_obj)\n","\n","def remove_hashtags(text):\n","    # Remove hashtags from the string text:\n","    text = hashtag_pattern.sub(r'\\1', text)\n","    return text\n","\n","def find_all_non_ascii(text):\n","    match_obj = non_ascii_pattern.findall(text)\n","    matches['non_ascii'] += len(match_obj)\n","\n","def remove_non_ascii(text):\n","    # Remove non-ASCII characters from the string text\n","    text = non_ascii_pattern.sub(r'', text)\n","    return text\n","\n","def find_all_multiples(text):\n","    match_obj_1 = multiples_pattern_1.findall(text)\n","    matches['multiples_!'] += len(match_obj_1)\n","\n","    match_obj_2 = multiples_pattern_2.findall(text)\n","    matches['multiples_?'] += len(match_obj_2)\n","\n","def remove_multiples(text):\n","    text = multiples_pattern_1.sub(r'!', text)\n","    text = multiples_pattern_2.sub(r'?', text)\n","    return text\n","\n","def find_all_stopwords(text):\n","    match_obj = stopwords_pattern.findall(text)\n","    matches['stop words'] += len(match_obj)\n","\n","def remove_stopwords(text):\n","    text = stopwords_pattern.sub('', text)\n","    return text\n","\n","def remove_punct(text):\n","    # Remove all punctuation characters except dot '.':\n","    return re.sub(r'[]!\"$%&\\'()*+,/:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n","\n","def find_all_the_rest(text):\n","    modif_obj = sample_typos_slang_pattern.findall(text)\n","    matches['typos'] += len(modif_obj)\n","\n","    modif_obj = sample_acronyms_pattern.findall(text)\n","    matches['acronyms'] += len(modif_obj)\n","\n","    modif_obj = sample_abbr_pattern.findall(text)\n","    matches['abbr'] += len(modif_obj)\n","\n","def remove_the_rest(text):\n","    text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n","    text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n","    text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n","    return text\n","\n","def determine_quantities(text):\n","    find_all_URL(text)\n","    find_all_html(text)\n","    find_all_spec_char(text)\n","    find_all_at(text)\n","    find_all_hashtags(text)\n","    find_all_non_ascii(text)\n","    find_all_multiples(text)\n","    find_all_stopwords(text)\n","    find_all_the_rest(text)\n","\n","def process_text(text):\n","    text = remove_URL(text)\n","    text = remove_html(text)\n","    text = remove_spec_char(text)\n","    text = remove_at(text)  # At symbols (@) with following ater it alphanumerical characters\n","    text = remove_hashtags(text)\n","    text = remove_non_ascii(text)\n","    text = remove_multiples(text) # Remove repeated characters '!' and '?'\n","    text = remove_stopwords(text)\n","#     text = remove_punct(text)\n","    text = text.lower()\n","    text = remove_the_rest(text)\n","    text = contractions.fix(text) # Remove english contractions with the 'contractions' module\n","    text = text.strip()\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:43:27.897173Z","iopub.execute_input":"2023-07-21T13:43:27.898597Z","iopub.status.idle":"2023-07-21T13:43:27.966102Z","shell.execute_reply.started":"2023-07-21T13:43:27.898531Z","shell.execute_reply":"2023-07-21T13:43:27.963671Z"},"trusted":true,"id":"Qb7N-wtjU1UI","executionInfo":{"status":"ok","timestamp":1690042320880,"user_tz":-420,"elapsed":468,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# for key in matches.keys():\n","#     matches[key] = 0\n","\n","# modified_text = process_text(\"BREAKING: Fairfax County firefighter placed on admin leave amid probe into Facebook post about putting police in 'body bags' dept. says.\")\n","# print(modified_text)\n","# for key in matches.keys():\n","#     print(f'В тестовом выражении найдено {key}\\'с: {matches[key]}.')\n","\n","text = 'BREAKING: Fairfax County firefighter placed on admin leave amid probe into Facebook post about putting police in \"body bags\" dept. says.'\n","pat = remove_stopwords(text)\n","print(pat)"],"metadata":{"trusted":true,"id":"I677SLE_U1UN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### We calculate now the quantities of all kinds of characteristic symbols in all the given tweets."],"metadata":{"id":"58epP5LIU1UN"}},{"cell_type":"code","source":["for key in matches.keys():\n","    matches[key] = 0\n","\n","train_tw.text.apply(determine_quantities)\n","\n","print('Found in the tweets:')\n","for key in matches.keys():\n","    print(f'{key}: {matches[key]}')"],"metadata":{"trusted":true,"id":"0Mnfo2loU1UP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Let us remove all or some of them."],"metadata":{"id":"i0jXUNTKU1UQ"}},{"cell_type":"code","source":["train_tw['processed_text'] = train_tw.text.apply(process_text)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:44:06.333334Z","iopub.execute_input":"2023-07-21T13:44:06.333736Z","iopub.status.idle":"2023-07-21T13:44:06.759201Z","shell.execute_reply.started":"2023-07-21T13:44:06.333709Z","shell.execute_reply":"2023-07-21T13:44:06.757468Z"},"trusted":true,"id":"ZhanHUAiU1UQ","executionInfo":{"status":"ok","timestamp":1690042422085,"user_tz":-420,"elapsed":934,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["for row in range(1017, 1042):\n","    print('Original tweet:')\n","    print (train_tw.loc[row, 'text'])\n","    print('-'*50)\n","    print('Cleaned tweet:')\n","    print(train_tw.loc[row, 'processed_text'])\n","    print('*'*50)"],"metadata":{"trusted":true,"id":"dG5iC5VoU1UQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Transforming the type of all the cells in the columns 'keyword' and 'location' to str."],"metadata":{"id":"DowI-SxHU1UR"}},{"cell_type":"code","source":["train_tw.keyword = train_tw['keyword'].astype('str')\n","train_tw.location = train_tw['location'].astype('str')"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:44:12.204557Z","iopub.execute_input":"2023-07-21T13:44:12.204993Z","iopub.status.idle":"2023-07-21T13:44:12.214410Z","shell.execute_reply.started":"2023-07-21T13:44:12.204968Z","shell.execute_reply":"2023-07-21T13:44:12.212980Z"},"trusted":true,"id":"l-vZAopEU1UR","executionInfo":{"status":"ok","timestamp":1690042428476,"user_tz":-420,"elapsed":492,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["#### Let's remove rows with empty cells. And in one case, we will try to remove from the data observations without a specified keyword. In another cas, we will try to leave them."],"metadata":{"id":"UVd9NiA8U1US"}},{"cell_type":"code","source":["train_tw = train_tw[train_tw['keyword'] != 'nan']\n","train_tw.dropna(subset='target')\n","train_tw.dropna(subset='text').reset_index(drop=True, inplace=True)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:44:16.752214Z","iopub.execute_input":"2023-07-21T13:44:16.752740Z","iopub.status.idle":"2023-07-21T13:44:16.786350Z","shell.execute_reply.started":"2023-07-21T13:44:16.752701Z","shell.execute_reply":"2023-07-21T13:44:16.784743Z"},"trusted":true,"id":"5ACxtPWOU1US","executionInfo":{"status":"ok","timestamp":1690042432975,"user_tz":-420,"elapsed":416,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["train_tw.shape"],"metadata":{"trusted":true,"id":"VDVX2P5WU1UT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Let's see now tweets' lengths after their processing."],"metadata":{"id":"qusetASBU1UV"}},{"cell_type":"code","source":["train_tw['new_tweet_length'] = [len(tweet.split()) for tweet in train_tw.processed_text]\n","print(train_tw['new_tweet_length'].quantile([0.01, 0.05, 0.10, 0.5, 0.9, 0.95, 0.99]))"],"metadata":{"trusted":true,"id":"U42M0pG0U1UV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_tweets_length_distrib('processed_text')"],"metadata":{"trusted":true,"id":"BT7vQx5NU1UW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.isna().any()"],"metadata":{"trusted":true,"id":"oolTSid4U1Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.reset_index(drop=True, inplace=True)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:44:26.436005Z","iopub.execute_input":"2023-07-21T13:44:26.436514Z","iopub.status.idle":"2023-07-21T13:44:26.444243Z","shell.execute_reply.started":"2023-07-21T13:44:26.436479Z","shell.execute_reply":"2023-07-21T13:44:26.442414Z"},"trusted":true,"id":"j2d3E2IkU1Uv","executionInfo":{"status":"ok","timestamp":1690042447619,"user_tz":-420,"elapsed":472,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["train_tw.head()"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:44:29.710586Z","iopub.execute_input":"2023-07-21T13:44:29.711044Z","iopub.status.idle":"2023-07-21T13:44:29.739724Z","shell.execute_reply.started":"2023-07-21T13:44:29.711013Z","shell.execute_reply":"2023-07-21T13:44:29.738600Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"O5O0QEYYU1Uw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### We add FastText vectors for words in the tweets."],"metadata":{"id":"yso8YlIRU1U0"}},{"cell_type":"code","source":["import fasttext.util\n","\n","fasttext.util.download_model('en', if_exists='ignore')  # English\n","ft = fasttext.load_model('cc.en.300.bin')"],"metadata":{"trusted":true,"id":"_MZ80t5MU1U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ft.get_word_vector('he'))"],"metadata":{"trusted":true,"id":"HPRTeMWwU1U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VECTOR_SIZE = ft.get_word_vector('hello')"],"metadata":{"trusted":true,"id":"2js6hoQtU1U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_vectors(text):\n","    vectors = {}\n","    words_list = text.split(' ')\n","    for word in words_list:\n","        vectors[word] = ft.get_word_vector(word)\n","    return vectors"],"metadata":{"trusted":true,"id":"EETrqceoU1U1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's add to the data a column with FastText vectrors for every word."],"metadata":{"id":"_kyw62_MU1U2"}},{"cell_type":"code","source":["train_tw['FastText vectors of words'] = train_tw.processed_text.apply(get_vectors)"],"metadata":{"trusted":true,"id":"onOxs2FOU1U2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.loc[7551, 'FastText vectors of words']"],"metadata":{"trusted":true,"id":"BsjkE-53U1U2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's save this table and download it later for models."],"metadata":{"id":"YJtoOHTmU1U3"}},{"cell_type":"code","source":["train_tw.to_pickle('Cleaned_data_with_fasttext_vectors.pkl',\n","                   compression={'method': 'gzip'})"],"metadata":{"trusted":true,"id":"Af0EUdUmU1U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tw.to_csv('Cleaned_data_with_fasttext_vectors.csv')"],"metadata":{"trusted":true,"id":"UOgUZIxDU1U3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation of classifiers' results."],"metadata":{"id":"hCIyl1AXU1U4"}},{"cell_type":"code","source":["def plot_conf_matrix(y_test, y_preds, save=False):\n","\n","    cm = confusion_matrix(y_test, y_preds)\n","    fig, ax = plt.subplots()\n","    tick_marks = np.asarray([0, 1])\n","    plt.xticks(tick_marks)\n","    plt.yticks(tick_marks)\n","    # create heatmap\n","    sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"BuPu\" ,fmt='g')\n","    ax.xaxis.set_label_position(\"top\")\n","    plt.tight_layout()\n","    plt.title('Confusion matrix', y=1.1)\n","    plt.ylabel('Actual label')\n","    plt.xlabel('Predicted label')\n","\n","    if save==True:\n","        save_plot(fig)\n","\n","    plt.show()\n","\n","\n","def plot_roc_auc(y_test, y_preds_proba, save=False):\n","\n","    model_auc = roc_auc_score(y_test, y_preds_proba, multi_class='ovr',\n","                              average='weighted')\n","    print(f'ROC AUC={model_auc:.4f}')\n","    # Let's compute ROC AUC\n","    fpr = {} # False Positive Rate\n","    tpr = {} # True Positive Rate\n","    thresh ={}\n","    n_class = y_preds_proba.shape[1] # 2 classes: disaster tweets and calm ones\n","\n","    colors = ['tab:blue', 'tab:orange']\n","    plt.figure(figsize=(8, 6))\n","    for i in range(n_class):\n","        fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_preds_proba[:,i], pos_label=i)\n","        plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i],\n","                 label=f'{i} vs Rest (area = {auc(fpr[i], tpr[i]):0.2f})')\n","\n","    plt.plot([0, 1], [0, 1], color='tab:gray', linestyle='--')\n","    plt.title('ROC curve')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive rate')\n","    plt.legend(loc='best')\n","\n","    if save==True:\n","        save_plot(fig)\n","\n","    plt.show()\n","\n","def save_plot(fig):\n","    now_time = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n","    current_dir = os.getcwd()\n","    # Let's create a new folder to save the graph\n","    new_path = os.path.join(current_dir, 'results')\n","    os.makedirs(new_path, exist_ok=True)\n","    file_path = os.path.join(new_path, 'results', f'conf_matrix {now_time}.png')\n","\n","    fig.savefig(file_path)"],"metadata":{"trusted":true,"id":"iFQpA1NkU1U4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_results(y_test, y_preds, y_preds_proba, save=False):\n","\n","    plot_conf_matrix(y_test, y_preds, save=save)\n","#     plot_roc_auc(y_test, y_preds_proba, save=save)\n","\n","    print(classification_report(y_test, y_preds))\n","    print(f'Accuracy: {accuracy_score(y_test, y_preds)}')\n","    print(f'f1_score: {f1_score(y_test, y_preds, average=\"weighted\")}')"],"metadata":{"trusted":true,"id":"BdM6xWPyU1U4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training classifiers and classifying test tweets."],"metadata":{"id":"2Qi9Pb7pU1U5"}},{"cell_type":"markdown","source":["#### Splitting data to train and validation sets:"],"metadata":{"id":"JsMVZg8nU1U6"}},{"cell_type":"code","source":["# import gensim.downloader as api\n","\n","# glove_vectors = api.load('glove-twitter-200')\n","# print(glove_vectors.get_vector('tweeting'))"],"metadata":{"trusted":true,"id":"W_Hdswc0U1U6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SENTENCE_VECTOR_TYPE = 'mean'  # 'weighted'\n","VECTOR_SIZE = 300\n","\n","def mean_sentence_vector():\n","    summarized_vector = np.zeros(VECTOR_SIZE)\n","    sentences_vectors = []\n","\n","    limit = len(train_tw)\n","    for row in range(limit):\n","        words_vectors_dict = train_tw.loc[row, 'FastText vectors of words']\n","        for vector in words_vectors_dict.values():\n","            summarized_vector += vector\n","        number = len(words_vectors_dict.values())\n","        sentence_vector = summarized_vector / number\n","\n","        sentences_vectors.append(sentence_vector)\n","\n","    train_tw['sentence vector'] = sentences_vectors\n","\n","def weighted_sentence_vector():\n","\n","    corpus = train_tw.processed_text\n","    vectorizer = TfidfVectorizer(token_pattern=r'(?!<\\w)([a-zA-Z]+)(?!\\w)')\n","    tfidf_vectors = vectorizer.fit_transform(corpus)\n","    # token_pattern=r'(?!<\\w)([a-zA-Z]+)(?!\\w)'\n","\n","    # print(len(vectorizer.get_feature_names_out()))\n","    print(f'The array of Tf-idf words sentences has the shape: {tfidf_vectors.shape}.')\n","\n","    vectors_df = pd.DataFrame(tfidf_vectors.toarray(),\n","                          columns = vectorizer.get_feature_names_out()).T\n","    print(vectors_df)\n","\n","    limit = len(train_tw)\n","    for row in range(limit):\n","        words_list = train_tw['FastText vectors of words']\n","        sentence_vector = np.zeros(VECTOR_SIZE)\n","\n","        for word in words_list.keys():\n","            sentence_vector += words_list['word'] * vectors_df.loc[word, row]\n","\n","        train_tw['sentence_vector'] = sentence_vector\n","\n","def vectorize_sentences():\n","    if SENTENCE_VECTOR_TYPE == 'mean':\n","        mean_sentence_vector()\n","#         return train_tw['FastText vectors of words'].apply(mean_sentence_vector)\n","    else:\n","        return weighted_sentence_vector()"],"metadata":{"trusted":true,"id":"3vD3qj9ZU1VT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_sentences()"],"metadata":{"trusted":true,"id":"WZHWpUSfU1VU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(train_tw.loc[19, 'sentence vector'].tolist()))"],"metadata":{"trusted":true,"id":"LVDoCW-DU1VU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = train_tw['sentence vector'].tolist()\n","Y = train_tw.target\n","x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42,\n","                                                    shuffle=True)"],"metadata":{"trusted":true,"id":"lHWQx5J1U1VV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Logistic regression."],"metadata":{"id":"gDp5GdihU1VX"}},{"cell_type":"code","source":["lr = LogisticRegression(random_state=42)\n","lr.fit(x_train, y_train)\n","y_preds = lr.predict(x_val)\n","y_preds_proba = lr.predict_proba(x_val)\n","evaluate_results(y_val, y_preds, y_preds_proba)"],"metadata":{"trusted":true,"id":"8Adu5v2yU1VZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Now we will find the optimal parameters for LogisticRegression with the help of GridSearchCV."],"metadata":{"id":"xbStt2RyU1VZ"}},{"cell_type":"code","source":["# Function which will print found parameters' values during searching the best ones.\n","\n","def print_dataframe(filtered_cv_results):\n","    \"\"\"Pretty print for filtered dataframe\"\"\"\n","    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n","        filtered_cv_results[\"mean_test_precision\"],\n","        filtered_cv_results[\"std_test_precision\"],\n","        filtered_cv_results[\"mean_test_recall\"],\n","        filtered_cv_results[\"std_test_recall\"],\n","        filtered_cv_results[\"params\"],\n","    ):\n","        print(\n","            f\"precision: {mean_precision:0.3f} (±{std_precision:0.03f}),\\n\"\n","            f\" recall: {mean_recall:0.3f} (±{std_recall:0.03f}),\\n\"\n","            f\" for {params}\"\n","        )\n","    print()\n","\n","# Function with the strategy of choosing the best parameters due to found metrics'\n","# values.\n","\n","def refit_estimator(cv_results):\n","    \"\"\"Define the strategy to select the best estimator.\n","\n","    The strategy defined here is to filter-out all results except results with\n","    maximum precision, rank the remaining by recall and keep all models within one\n","    standarddeviation of the best by recall. Once these models are selected, we\n","    select the fastest model to predict.\n","\n","    Parameters\n","    ----------\n","    cv_results : dict of numpy (masked) ndarrays\n","        CV results as returned by the `GridSearchCV`.\n","\n","    Returns\n","    -------\n","    best_index : int\n","        The index of the best estimator as it appears in `cv_results`.\n","    \"\"\"\n","    # print the info about the grid-search for the different scores\n","\n","    cv_results_ = pd.DataFrame(cv_results)\n","    print(\"All grid-search results:\")\n","    print_dataframe(cv_results_)\n","\n","    precision_threshold = cv_results_[\"mean_test_precision\"].max()\n","\n","    # Filter-out all results below the threshold\n","    high_precision_cv_results = cv_results_[\n","        cv_results_[\"mean_test_precision\"] == precision_threshold\n","    ]\n","\n","    print(f\"Models with a precision higher than {precision_threshold}:\")\n","    print_dataframe(high_precision_cv_results)\n","\n","    high_precision_cv_results = high_precision_cv_results[\n","        [\n","            \"mean_score_time\",\n","            \"mean_test_recall\",\n","            \"std_test_recall\",\n","            \"mean_test_precision\",\n","            \"std_test_precision\",\n","            \"rank_test_recall\",\n","            \"rank_test_precision\",\n","            \"params\",\n","        ]\n","    ]\n","\n","    # Select the most performant models in terms of recall\n","    # (within 1 sigma from the best)\n","    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n","    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n","    best_recall_threshold = best_recall - best_recall_std\n","\n","    high_recall_cv_results = high_precision_cv_results[\n","        high_precision_cv_results[\"mean_test_recall\"] >= best_recall_threshold\n","    ]\n","    print(\n","        \"Out of the previously selected high precision models, we keep all the\\n\"\n","        \"the models within one standard deviation of the highest recall model:\"\n","    )\n","    print_dataframe(high_recall_cv_results)\n","\n","    # From the best candidates, select the fastest model to predict\n","    fastest_top_recall_high_precision_index = high_recall_cv_results[\n","        \"mean_score_time\"\n","    ].idxmin()\n","\n","    print(\n","        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n","        \"selected subset of best models based on precision and recall.\\n\"\n","        \"Its scoring time is:\\n\\n\"\n","        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n","    )\n","\n","    print(f'Best parameters:\\n'\n","         f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index, 'params']}\")\n","\n","    return fastest_top_recall_high_precision_index"],"metadata":{"trusted":true,"id":"i--6U_uTU1Vc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_params_grid = [{\n","        'penalty': ('l1', 'l2', 'elasticnet'),\n","        'tol': (1e-3, 1e-4),\n","        'C': (1.0, 10.0, 50.0, 100.0),\n","        'solver': ('saga', 'lbfgs'),\n","        'max_iter': (100, 250, 50)\n","        },\n","        {'penalty': ['l2'],\n","         'tol': (1e-3, 1e-4),\n","         'C': (1.0, 10.0, 50.0, 100.0),\n","         'solver': ('sag', 'lbfgs', 'newton-cg', 'newton-cholesky'),\n","         'max_iter': (100, 250, 50)\n","        }\n","]\n","lr = LogisticRegression(random_state=42)\n","scores = [\"precision\", \"recall\"]\n","\n","clf = GridSearchCV(estimator=lr, param_grid=lr_params_grid, scoring=scores,\n","                  refit=refit_estimator)"],"metadata":{"trusted":true,"id":"yu-nCuacU1Vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf.fit(x_train, y_train)"],"metadata":{"trusted":true,"id":"1lLUnRuMU1Vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(clf.best_params_)\n","print(clf.best_estimator_)"],"metadata":{"trusted":true,"id":"HJR-Sq9dU1Vh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_preds = clf.predict(x_val)\n","y_preds_proba = clf.predict_proba(x_val)\n","evaluate_results(y_val, y_preds, y_preds_proba)"],"metadata":{"trusted":true,"id":"yBugdGuIU1Vh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Let us try to use a pre-trained model \"Twitter-roBERTa-base-sentiment\" for Sentiment Analysis."],"metadata":{"id":"kDAf7XVWU1Vi"}},{"cell_type":"code","source":["from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    PretrainedConfig,\n","    SchedulerType,\n","    default_data_collator,\n","    get_scheduler,\n",")"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:44:47.158675Z","iopub.execute_input":"2023-07-21T13:44:47.159147Z","iopub.status.idle":"2023-07-21T13:44:47.197679Z","shell.execute_reply.started":"2023-07-21T13:44:47.159120Z","shell.execute_reply":"2023-07-21T13:44:47.196731Z"},"trusted":true,"id":"HclCecNJU1Vi","executionInfo":{"status":"ok","timestamp":1690042486794,"user_tz":-420,"elapsed":623,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["NUM_LABELS = train_tw.target.nunique()\n","BATCH_SIZE = 64\n","NUM_EPOCHS = 10\n","LR         = 1e-3\n","MODEL      = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","config = AutoConfig.from_pretrained(MODEL, num_labels=NUM_LABELS)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, config=config,\n","                            ignore_mismatched_sizes=True,)\n","# Let's see the maximum number of words in a sequence which can be processed\n","# by the model.\n","# model.model_max_length"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:45:41.196540Z","iopub.execute_input":"2023-07-21T13:45:41.197052Z","iopub.status.idle":"2023-07-21T13:45:46.728659Z","shell.execute_reply.started":"2023-07-21T13:45:41.197014Z","shell.execute_reply":"2023-07-21T13:45:46.727207Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"C8jjEcE3U1Vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_function(text):\n","    return tokenizer(text, truncation=True)\n","\n","train_tw['tokenized_text'] = train_tw.processed_text.apply(tokenize_function)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:46:45.977405Z","iopub.execute_input":"2023-07-21T13:46:45.977792Z","iopub.status.idle":"2023-07-21T13:46:46.897791Z","shell.execute_reply.started":"2023-07-21T13:46:45.977764Z","shell.execute_reply":"2023-07-21T13:46:46.895825Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"VOtKgunXU1Vo","executionInfo":{"status":"ok","timestamp":1690042511688,"user_tz":-420,"elapsed":2249,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}},"outputId":"a3e5d9a6-973d-4e89-f438-0293479ad20a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}]},{"cell_type":"code","source":["X = train_tw.tokenized_text\n","Y = train_tw.target\n","x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.3,\n","                                                  random_state=42, shuffle=True)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:46:51.821937Z","iopub.execute_input":"2023-07-21T13:46:51.822380Z","iopub.status.idle":"2023-07-21T13:46:51.833932Z","shell.execute_reply.started":"2023-07-21T13:46:51.822352Z","shell.execute_reply":"2023-07-21T13:46:51.831765Z"},"trusted":true,"id":"ATB_fCT2U1Vo","executionInfo":{"status":"ok","timestamp":1690042516272,"user_tz":-420,"elapsed":434,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_preds):\n","\n","    y_test, y_preds = eval_preds\n","    plot_conf_matrix(y_test, y_preds)\n","#     plot_roc_auc(y_test, y_preds_proba, save=save)\n","\n","    print(classification_report(y_test, y_preds))\n","    print(f'Accuracy: {accuracy_score(y_test, y_preds)}')\n","    print(f'f1_score: {f1_score(y_test, y_preds, average=\"weighted\")}')"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:46:57.386338Z","iopub.execute_input":"2023-07-21T13:46:57.386801Z","iopub.status.idle":"2023-07-21T13:46:57.393822Z","shell.execute_reply.started":"2023-07-21T13:46:57.386760Z","shell.execute_reply":"2023-07-21T13:46:57.392560Z"},"trusted":true,"id":"RwHEc76KU1Vt","executionInfo":{"status":"ok","timestamp":1690042523162,"user_tz":-420,"elapsed":517,"user":{"displayName":"Алексей Тяжев","userId":"10697695251284968559"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:53:22.774033Z","iopub.execute_input":"2023-07-21T13:53:22.774398Z","iopub.status.idle":"2023-07-21T13:53:32.813954Z","shell.execute_reply.started":"2023-07-21T13:53:22.774372Z","shell.execute_reply":"2023-07-21T13:53:32.811980Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"RM87C9-sU1Vy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","# To use the 'Trainer' with 'PyTorch' Google Colab requires 'accelerate>=0.20.1'.\n","# For that, it's necessary to run '!pip install transformers[torch]' or '!pip install accelerate -U'\n","!pip install accelerate -U"],"metadata":{"trusted":true,"id":"XZQLbll3U1Vy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install 'accelerate>=0.20.3'"],"metadata":{"id":"4XwDMhLIbCFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers[torch]"],"metadata":{"id":"WbkkTePmZbba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    \"test-trainer\",\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=32,\n","    warmup_steps=500,\n","    weight_decay=0.01\n",")\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=x_train,\n","    eval_dataset=x_val,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics, # Это опциально - если мы хотим считать метрики. Если мы не хотим считать метрики, то мы не задаем этот параметр.\n",")\n","\n","# evaluate_results(y_test, y_preds, y_preds_proba, save=False)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-21T13:49:22.521783Z","iopub.execute_input":"2023-07-21T13:49:22.522393Z","iopub.status.idle":"2023-07-21T13:49:23.712911Z","shell.execute_reply.started":"2023-07-21T13:49:22.522352Z","shell.execute_reply":"2023-07-21T13:49:23.710750Z"},"trusted":true,"id":"YkBaWYSuU1Vy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"fgl_AmHKU1Vz"},"execution_count":null,"outputs":[]}]}