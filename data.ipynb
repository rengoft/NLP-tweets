{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions\nimport contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate\nimport evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    roc_auc_score,\n    roc_curve,\n    auc,\n    confusion_matrix,\n    classification_report,\n)\n\nfrom nltk.corpus import stopwords\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Uploading train data","metadata":{}},{"cell_type":"code","source":"train_tw = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"### Looking at the data","metadata":{}},{"cell_type":"code","source":"train_tw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.keyword.value_counts().index.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"locations = train_tw.location.value_counts().index.to_list()\nprint(locations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print (train_tw.loc[3107: 3142, 'text'])\nfor row in range(3107, 3142):\n    print (train_tw.loc[row, 'text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let us now see the distribution of the tweets lengths in quantiles.","metadata":{}},{"cell_type":"code","source":"train_tw['tweet_length'] = [len(tweet.split()) for tweet in train_tw.text]\ntrain_tw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.tweet_length.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.tweet_length.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_tw['tweet_length'].quantile([0.01, 0.05, 0.10, 0.5, 0.9, 0.95, 0.99]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_tweets_length_distrib(column):\n\n    fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n    fig = plt.figure(figsize=(5, 10))\n\n    lengths = [len(tweet.split()) for tweet in train_tw[column]]\n    ax[0].hist(lengths, bins=31, linewidth=0.5, edgecolor=\"white\")\n\n    lengths = [len(tweet.split()) for tweet in train_tw[train_tw['target'] == 1][column]]\n    ax[1].hist(lengths, bins=31, linewidth=0.5, edgecolor=\"white\")\n\n    lengths = [len(tweet.split()) for tweet in train_tw[train_tw['target'] == 0][column]]\n    ax[2].hist(lengths, bins=31, linewidth=0.5, edgecolor=\"white\")\n\n    names = ['Common distribution', 'Distribution of disaster tweets.',\n             'Distribution of calm tweets.']\n    for i in range(len(names)):\n        ax[i].set_title(names[i])\n        ax[i].set_xlabel('Length')\n        ax[i].set_ylabel('Number of tweets')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_tweets_length_distrib('text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Processing tweets. Let us try to find in the tweets characteristic symbols, like as URL's, html's, special characters, @ etc. In case some of them are found, we remove them.","metadata":{}},{"cell_type":"code","source":"# We introduce counters for every matching found to see how many of them are found:\nmatches = {'URL': 0, 'html': 0, 'spec_char': 0, 'at': 0, 'hashtags': 0, 'non_ascii': 0,\n           'multiples_!': 0, 'multiples_?': 0, 'stop words': 0,\n           'typos': 0, 'acronyms': 0, 'abbr': 0}\n        \n# Patterns for URL's\nhttps_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n# Patterns for html's\nhtml_pattern = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n# Patterns for special characters\nemoji_pattern = re.compile(\n                            '['\n                            u'\\U0001F600-\\U0001F64F'  # emoticons\n                            u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n                            u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n                            u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n                            u'\\U00002702-\\U000027B0'\n                            u'\\U000024C2-\\U0001F251'\n                            ']+',\n                            flags=re.UNICODE\n    )\n# Patterns for character sequences with At symbol (@)\nat_pattern = re.compile(r'@\\S*')\n# Patterns for hashtag symbols\nhashtag_pattern = re.compile(r'#([^\\s]+)')\n# Patterns for non_ascii characters:\nnon_ascii_pattern = re.compile(r'[^\\x00-\\x7f]')\n# Patterns for sequences of several '!'\nmultiples_pattern_1 = re.compile(r'!+')\n# Patterns for sequences of several '?'\nmultiples_pattern_2 = re.compile(r'\\?+')\n\n# Pattern for english stopwords\nstopwords_list = stopwords.words('english')\nstopwords_pattern = re.compile(r'(?<!\\w)(' + '|'.join(word \\\n                        for word in stopwords_list) + r')(?!\\w)')\n\n# Taking into account typos, slang and other:\nsample_typos_slang = {\n                        \"w/e\": \"whatever\",\n                        \"usagov\": \"usa government\",\n                        \"recentlu\": \"recently\",\n                        \"ph0tos\": \"photos\",\n                        \"amirite\": \"am i right\",\n                        \"exp0sed\": \"exposed\",\n                        \"<3\": \"love\",\n                        \"luv\": \"love\",\n                        \"amageddon\": \"armageddon\",\n                        \"trfc\": \"traffic\",\n                        \"16yr\": \"16 year\"\n                        }\nsample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) \\\n                            for key in sample_typos_slang.keys()) + r')(?!\\w)')\n\n# Acronyms\nsample_acronyms =  { \n                    \"mh370\": \"malaysia airlines flight 370\",\n                    \"okwx\": \"oklahoma city weather\",\n                    \"arwx\": \"arkansas weather\",    \n                    \"gawx\": \"georgia weather\",  \n                    \"scwx\": \"south carolina weather\",  \n                    \"cawx\": \"california weather\",\n                    \"tnwx\": \"tennessee weather\",\n                    \"azwx\": \"arizona weather\",  \n                    \"alwx\": \"alabama weather\",\n                    \"usnwsgov\": \"united states national weather service\",\n                    \"2mw\": \"tomorrow\"\n                    }\nsample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) \\\n                            for key in sample_acronyms.keys()) + r')(?!\\w)')\n\n# Some common english abbreviations: \nsample_abbr = {     \"$\" : \" dollar \",\n                    \"€\" : \" euro \",\n                    \"4ao\" : \"for adults only\",\n                    \"a.m\" : \"before midday\",\n                    \"a3\" : \"anytime anywhere anyplace\",\n                    \"aamof\" : \"as a matter of fact\",\n                    \"acct\" : \"account\",\n                    \"adih\" : \"another day in hell\",\n                    \"afaic\" : \"as far as i am concerned\",\n                    \"afaict\" : \"as far as i can tell\",\n                    \"afaik\" : \"as far as i know\",\n                    \"afair\" : \"as far as i remember\",\n                    \"afk\" : \"away from keyboard\",\n                    \"app\" : \"application\",\n                    \"approx\" : \"approximately\",\n                    \"apps\" : \"applications\",\n                    \"asap\" : \"as soon as possible\",\n                    \"asl\" : \"age, sex, location\",\n                    \"atk\" : \"at the keyboard\",\n                    \"ave.\" : \"avenue\",\n                    \"aymm\" : \"are you my mother\",\n                    \"ayor\" : \"at your own risk\", \n                    \"b&b\" : \"bed and breakfast\",\n                    \"b+b\" : \"bed and breakfast\",\n                    \"b.c\" : \"before christ\",\n                    \"b2b\" : \"business to business\",\n                    \"b2c\" : \"business to customer\",\n                    \"b4\" : \"before\",\n                    \"b4n\" : \"bye for now\",\n                    \"b@u\" : \"back at you\",\n                    \"bae\" : \"before anyone else\",\n                    \"bak\" : \"back at keyboard\",\n                    \"bbbg\" : \"bye bye be good\",\n                    \"bbc\" : \"british broadcasting corporation\",\n                    \"bbias\" : \"be back in a second\",\n                    \"bbl\" : \"be back later\",\n                    \"bbs\" : \"be back soon\",\n                    \"be4\" : \"before\",\n                    \"bfn\" : \"bye for now\",\n                    \"blvd\" : \"boulevard\",\n                    \"bout\" : \"about\",\n                    \"brb\" : \"be right back\",\n                    \"bros\" : \"brothers\",\n                    \"brt\" : \"be right there\",\n                    \"bsaaw\" : \"big smile and a wink\",\n                    \"btw\" : \"by the way\",\n                    \"bwl\" : \"bursting with laughter\",\n                    \"c/o\" : \"care of\",\n                    \"cet\" : \"central european time\",\n                    \"cf\" : \"compare\",\n                    \"cia\" : \"central intelligence agency\",\n                    \"csl\" : \"can not stop laughing\",\n                    \"cu\" : \"see you\",\n                    \"cul8r\" : \"see you later\",\n                    \"cv\" : \"curriculum vitae\",\n                    \"cwot\" : \"complete waste of time\",\n                    \"cya\" : \"see you\",\n                    \"cyt\" : \"see you tomorrow\",\n                    \"dae\" : \"does anyone else\",\n                    \"dbmib\" : \"do not bother me i am busy\",\n                    \"diy\" : \"do it yourself\",\n                    \"dm\" : \"direct message\",\n                    \"dwh\" : \"during work hours\",\n                    \"e123\" : \"easy as one two three\",\n                    \"eet\" : \"eastern european time\",\n                    \"eg\" : \"example\",\n                    \"embm\" : \"early morning business meeting\",\n                    \"encl\" : \"enclosed\",\n                    \"encl.\" : \"enclosed\",\n                    \"etc\" : \"and so on\",\n                    \"faq\" : \"frequently asked questions\",\n                    \"fawc\" : \"for anyone who cares\",\n                    \"fb\" : \"facebook\",\n                    \"fc\" : \"fingers crossed\",\n                    \"fig\" : \"figure\",\n                    \"fimh\" : \"forever in my heart\", \n                    \"ft.\" : \"feet\",\n                    \"ft\" : \"featuring\",\n                    \"ftl\" : \"for the loss\",\n                    \"ftw\" : \"for the win\",\n                    \"fwiw\" : \"for what it is worth\",\n                    \"fyi\" : \"for your information\",\n                    \"g9\" : \"genius\",\n                    \"gahoy\" : \"get a hold of yourself\",\n                    \"gal\" : \"get a life\",\n                    \"gcse\" : \"general certificate of secondary education\",\n                    \"gfn\" : \"gone for now\",\n                    \"gg\" : \"good game\",\n                    \"gl\" : \"good luck\",\n                    \"glhf\" : \"good luck have fun\",\n                    \"gmt\" : \"greenwich mean time\",\n                    \"gmta\" : \"great minds think alike\",\n                    \"gn\" : \"good night\",\n                    \"g.o.a.t\" : \"greatest of all time\",\n                    \"goat\" : \"greatest of all time\",\n                    \"goi\" : \"get over it\",\n                    \"gps\" : \"global positioning system\",\n                    \"gr8\" : \"great\",\n                    \"gratz\" : \"congratulations\",\n                    \"gyal\" : \"girl\",\n                    \"h&c\" : \"hot and cold\",\n                    \"hp\" : \"horsepower\",\n                    \"hr\" : \"hour\",\n                    \"hrh\" : \"his royal highness\",\n                    \"ht\" : \"height\",\n                    \"ibrb\" : \"i will be right back\",\n                    \"ic\" : \"i see\",\n                    \"icq\" : \"i seek you\",\n                    \"icymi\" : \"in case you missed it\",\n                    \"idc\" : \"i do not care\",\n                    \"idgadf\" : \"i do not give a damn fuck\",\n                    \"idgaf\" : \"i do not give a fuck\",\n                    \"idk\" : \"i do not know\",\n                    \"ie\" : \"that is\",\n                    \"i.e\" : \"that is\",\n                    \"ifyp\" : \"i feel your pain\",\n                    \"IG\" : \"instagram\",\n                    \"iirc\" : \"if i remember correctly\",\n                    \"ilu\" : \"i love you\",\n                    \"ily\" : \"i love you\",\n                    \"imho\" : \"in my humble opinion\",\n                    \"imo\" : \"in my opinion\",\n                    \"imu\" : \"i miss you\",\n                    \"iow\" : \"in other words\",\n                    \"irl\" : \"in real life\",\n                    \"j4f\" : \"just for fun\",\n                    \"jic\" : \"just in case\",\n                    \"jk\" : \"just kidding\",\n                    \"jsyk\" : \"just so you know\",\n                    \"l8r\" : \"later\",\n                    \"lb\" : \"pound\",\n                    \"lbs\" : \"pounds\",\n                    \"ldr\" : \"long distance relationship\",\n                    \"lmao\" : \"laugh my ass off\",\n                    \"lmfao\" : \"laugh my fucking ass off\",\n                    \"lol\" : \"laughing out loud\",\n                    \"ltd\" : \"limited\",\n                    \"ltns\" : \"long time no see\",\n                    \"m8\" : \"mate\",\n                    \"mf\" : \"motherfucker\",\n                    \"mfs\" : \"motherfuckers\",\n                    \"mfw\" : \"my face when\",\n                    \"mofo\" : \"motherfucker\",\n                    \"mph\" : \"miles per hour\",\n                    \"mr\" : \"mister\",\n                    \"mrw\" : \"my reaction when\",\n                    \"ms\" : \"miss\",\n                    \"mte\" : \"my thoughts exactly\",\n                    \"nagi\" : \"not a good idea\",\n                    \"nbc\" : \"national broadcasting company\",\n                    \"nbd\" : \"not big deal\",\n                    \"nfs\" : \"not for sale\",\n                    \"ngl\" : \"not going to lie\",\n                    \"nhs\" : \"national health service\",\n                    \"nrn\" : \"no reply necessary\",\n                    \"nsfl\" : \"not safe for life\",\n                    \"nsfw\" : \"not safe for work\",\n                    \"nth\" : \"nice to have\",\n                    \"nvr\" : \"never\",\n                    \"nyc\" : \"new york city\",\n                    \"oc\" : \"original content\",\n                    \"og\" : \"original\",\n                    \"ohp\" : \"overhead projector\",\n                    \"oic\" : \"oh i see\",\n                    \"omdb\" : \"over my dead body\",\n                    \"omg\" : \"oh my god\",\n                    \"omw\" : \"on my way\",\n                    \"p.a\" : \"per annum\",\n                    \"p.m\" : \"after midday\",\n                    \"pm\" : \"prime minister\",\n                    \"poc\" : \"people of color\",\n                    \"pov\" : \"point of view\",\n                    \"pp\" : \"pages\",\n                    \"ppl\" : \"people\",\n                    \"prw\" : \"parents are watching\",\n                    \"ps\" : \"postscript\",\n                    \"pt\" : \"point\",\n                    \"ptb\" : \"please text back\",\n                    \"pto\" : \"please turn over\",\n                    \"qpsa\" : \"what happens\", #\"que pasa\",\n                    \"ratchet\" : \"rude\",\n                    \"rbtl\" : \"read between the lines\",\n                    \"rlrt\" : \"real life retweet\", \n                    \"rofl\" : \"rolling on the floor laughing\",\n                    \"roflol\" : \"rolling on the floor laughing out loud\",\n                    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n                    \"rt\" : \"retweet\",\n                    \"ruok\" : \"are you ok\",\n                    \"sfw\" : \"safe for work\",\n                    \"sk8\" : \"skate\",\n                    \"smh\" : \"shake my head\",\n                    \"sq\" : \"square\",\n                    \"srsly\" : \"seriously\", \n                    \"ssdd\" : \"same stuff different day\",\n                    \"tbh\" : \"to be honest\",\n                    \"tbs\" : \"tablespooful\",\n                    \"tbsp\" : \"tablespooful\",\n                    \"tfw\" : \"that feeling when\",\n                    \"thks\" : \"thank you\",\n                    \"tho\" : \"though\",\n                    \"thx\" : \"thank you\",\n                    \"tia\" : \"thanks in advance\",\n                    \"til\" : \"today i learned\",\n                    \"tl;dr\" : \"too long i did not read\",\n                    \"tldr\" : \"too long i did not read\",\n                    \"tmb\" : \"tweet me back\",\n                    \"tntl\" : \"trying not to laugh\",\n                    \"ttyl\" : \"talk to you later\",\n                    \"u.s.\" : 'usa',\n                    \"u\" : \"you\",\n                    \"u2\" : \"you too\",\n                    \"u4e\" : \"yours for ever\",\n                    \"utc\" : \"coordinated universal time\",\n                    \"w/\" : \"with\",\n                    \"w/o\" : \"without\",\n                    \"w8\" : \"wait\",\n                    \"wassup\" : \"what is up\",\n                    \"wb\" : \"welcome back\",\n                    \"wtf\" : \"what the fuck\",\n                    \"wtg\" : \"way to go\",\n                    \"wtpa\" : \"where the party at\",\n                    \"wuf\" : \"where are you from\",\n                    \"wuzup\" : \"what is up\",\n                    \"wywh\" : \"wish you were here\",\n                    \"yd\" : \"yard\",\n                    \"ygtr\" : \"you got that right\",\n                    \"ynk\" : \"you never know\",\n                    \"zzz\" : \"sleeping bored and tired\"\n                    }\nsample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) \\\n                            for key in sample_abbr.keys()) + r')(?!\\w)')\n\ndef find_all_URL(text):\n    match_obj = re.findall(https_pattern, text)\n    matches['URL'] += len(match_obj)\n\ndef remove_URL(text):\n    # Remove URL's from the string text:\n    text = re.sub(https_pattern, \"\", text)\n    return text\n\ndef find_all_html(text):\n    match_obj = re.findall(html_pattern, text)\n    matches['html'] += len(match_obj)\n\ndef remove_html(text):\n    # Remove html from the string text:\n    text = re.sub(html_pattern, \"\", text)\n    return text\n\ndef find_all_spec_char(text):\n    match_obj = re.findall(emoji_pattern, text)\n    matches['spec_char'] += len(match_obj)\n\ndef remove_spec_char(text):\n    # Remove special characters (like emojis and different graphic characters):\n    text = emoji_pattern.sub(r'', text)\n    return text\n\ndef find_all_at(text):\n    match_obj = at_pattern.findall(text)\n    matches['at'] += len(match_obj)\n\ndef remove_at(text):\n    text = at_pattern.sub(r'', text)\n    return text\n\ndef find_all_hashtags(text):\n    match_obj = hashtag_pattern.findall(text)\n    matches['hashtags'] += len(match_obj)\n\ndef remove_hashtags(text):\n    # Remove hashtags from the string text:\n    text = hashtag_pattern.sub(r'\\1', text)\n    return text\n\ndef find_all_non_ascii(text):\n    match_obj = non_ascii_pattern.findall(text)\n    matches['non_ascii'] += len(match_obj)\n\ndef remove_non_ascii(text):\n    # Remove non-ASCII characters from the string text\n    text = non_ascii_pattern.sub(r'', text)\n    return text\n\ndef find_all_multiples(text):\n    match_obj_1 = multiples_pattern_1.findall(text)\n    matches['multiples_!'] += len(match_obj_1)\n    \n    match_obj_2 = multiples_pattern_2.findall(text)\n    matches['multiples_?'] += len(match_obj_2)\n\ndef remove_multiples(text):\n    text = multiples_pattern_1.sub(r'!', text)\n    text = multiples_pattern_2.sub(r'?', text)\n    return text\n\ndef find_all_stopwords(text):\n    match_obj = stopwords_pattern.findall(text)\n    matches['stop words'] += len(match_obj)\n\ndef remove_stopwords(text):\n    text = stopwords_pattern.sub('', text)\n    return text\n\ndef remove_punct(text):\n    # Remove all punctuation characters except dot '.':\n    return re.sub(r'[]!\"$%&\\'()*+,/:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n\ndef find_all_the_rest(text):\n    modif_obj = sample_typos_slang_pattern.findall(text)\n    matches['typos'] += len(modif_obj)\n    \n    modif_obj = sample_acronyms_pattern.findall(text)\n    matches['acronyms'] += len(modif_obj)\n    \n    modif_obj = sample_abbr_pattern.findall(text)\n    matches['abbr'] += len(modif_obj)\n    \ndef remove_the_rest(text): \n    text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n    text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n    text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n    return text\n  \ndef determine_quantities(text):\n    find_all_URL(text)\n    find_all_html(text)\n    find_all_spec_char(text)\n    find_all_at(text)\n    find_all_hashtags(text)\n    find_all_non_ascii(text)\n    find_all_multiples(text)\n    find_all_stopwords(text)\n    find_all_the_rest(text)\n        \ndef process_text(text):\n    text = remove_URL(text)\n    text = remove_html(text)\n    text = remove_spec_char(text)\n    text = remove_at(text)  # At symbols (@) with following ater it alphanumerical characters\n    text = remove_hashtags(text)\n    text = remove_non_ascii(text)\n    text = remove_multiples(text) # Remove repeated characters '!' and '?'\n    text = remove_stopwords(text)\n#     text = remove_punct(text)\n    text = text.lower()\n    text = remove_the_rest(text)\n    text = contractions.fix(text) # Remove english contractions with the 'contractions' module\n    text = text.strip()\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for key in matches.keys():\n#     matches[key] = 0\n\n# modified_text = process_text(\"BREAKING: Fairfax County firefighter placed on admin leave amid probe into Facebook post about putting police in 'body bags' dept. says.\")\n# print(modified_text)\n# for key in matches.keys():\n#     print(f'В тестовом выражении найдено {key}\\'с: {matches[key]}.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'BREAKING: Fairfax County firefighter placed on admin leave amid probe into Facebook post about putting police in \"body bags\" dept. says.'\npat = remove_stopwords(text)\nprint(pat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We calculate now the quantities of all kinds of characteristic symbols in all the given tweets.","metadata":{}},{"cell_type":"code","source":"for key in matches.keys():\n    matches[key] = 0\n\ntrain_tw.text.apply(determine_quantities)\n\nprint('Found in the tweets:')\nfor key in matches.keys():\n    print(f'{key}: {matches[key]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let us remove all or some of them.","metadata":{}},{"cell_type":"code","source":"train_tw['processed_text'] = train_tw.text.apply(process_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for row in range(1017, 1042):\n    print('Original tweet:')\n    print (train_tw.loc[row, 'text'])\n    print('-'*50)\n    print('Cleaned tweet:')\n    print(train_tw.loc[row, 'processed_text'])\n    print('*'*50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transforming the type of all the cells in the columns 'keyword' and 'location' to str.","metadata":{}},{"cell_type":"code","source":"train_tw.keyword = train_tw['keyword'].astype('str')\ntrain_tw.location = train_tw['location'].astype('str')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's remove rows with empty cells. And in one case, we will try to remove from the data observations without a specified keyword. In another cas, we will try to leave them.","metadata":{}},{"cell_type":"code","source":"train_tw = train_tw[train_tw['keyword'] != 'nan']\ntrain_tw.dropna(subset='target')\ntrain_tw.dropna(subset='text').reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's see now tweets' lengths after their processing.","metadata":{}},{"cell_type":"code","source":"train_tw['new_tweet_length'] = [len(tweet.split()) for tweet in train_tw.processed_text]\nprint(train_tw['new_tweet_length'].quantile([0.01, 0.05, 0.10, 0.5, 0.9, 0.95, 0.99]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_tweets_length_distrib('processed_text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.isna().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We add FastText vectors for words in the tweets.","metadata":{}},{"cell_type":"code","source":"import fasttext.util\n\nfasttext.util.download_model('en', if_exists='ignore')  # English\nft = fasttext.load_model('cc.en.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ft.get_word_vector('he'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VECTOR_SIZE = ft.get_word_vector('hello')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_vectors(text):\n    vectors = {}\n    words_list = text.split(' ')\n    for word in words_list:\n        vectors[word] = ft.get_word_vector(word)\n    return vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's add to the data a column with FastText vectrors for every word.","metadata":{}},{"cell_type":"code","source":"train_tw['FastText vectors of words'] = train_tw.processed_text.apply(get_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.loc[7552, 'FastText vectors of words']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's save this table and download it later for models.","metadata":{}},{"cell_type":"code","source":"train_tw.to_pickle('Cleaned_data_with_fasttext_vectors.pkl',\n                   compression={'method': 'gzip'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tw.to_csv('Cleaned_data_with_fasttext_vectors.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation of classifiers' results.","metadata":{}},{"cell_type":"code","source":"def plot_conf_matrix(y_test, y_preds, save=False):\n    \n    cm = confusion_matrix(y_test, y_preds)\n    fig, ax = plt.subplots()\n    tick_marks = np.asarray([0, 1])\n    plt.xticks(tick_marks)\n    plt.yticks(tick_marks)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"BuPu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    \n    if save==True:\n        save_plot(fig)\n                \n    plt.show()\n\n    \ndef plot_roc_auc(y_test, y_preds_proba, save=False):\n\n    model_auc = roc_auc_score(y_test, y_preds_proba, multi_class='ovr',\n                              average='weighted')\n    print(f'ROC AUC={model_auc:.4f}')\n    # Let's compute ROC AUC\n    fpr = {} # False Positive Rate\n    tpr = {} # True Positive Rate\n    thresh ={}\n    n_class = y_preds_proba.shape[1] # 2 classes: disaster tweets and calm ones\n\n    colors = ['tab:blue', 'tab:orange']\n    plt.figure(figsize=(8, 6))\n    for i in range(n_class):    \n        fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_preds_proba[:,i], pos_label=i)\n        plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i],\n                 label=f'{i} vs Rest (area = {auc(fpr[i], tpr[i]):0.2f})')\n        \n    plt.plot([0, 1], [0, 1], color='tab:gray', linestyle='--')\n    plt.title('ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive rate')\n    plt.legend(loc='best')\n    \n    if save==True:\n        save_plot(fig)\n        \n    plt.show()\n\ndef save_plot(fig):\n    now_time = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n    current_dir = os.getcwd()\n    # Let's create a new folder to save the graph\n    new_path = os.path.join(current_dir, 'results')\n    os.makedirs(new_path, exist_ok=True)\n    file_path = os.path.join(new_path, 'results', f'conf_matrix {now_time}.png')\n\n    fig.savefig(file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_results(y_test, y_preds, y_preds_proba, save=False):\n    \n    plot_conf_matrix(y_test, y_preds, save=save)\n#     plot_roc_auc(y_test, y_preds_proba, save=save)\n        \n    print(classification_report(y_test, y_preds))\n    print(f'Accuracy: {accuracy_score(y_test, y_preds)}')\n    print(f'f1_score: {f1_score(y_test, y_preds, average=\"weighted\")}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training classifiers and classifying test tweets.","metadata":{}},{"cell_type":"markdown","source":"#### Splitting data to train and validation sets:","metadata":{}},{"cell_type":"code","source":"# import gensim.downloader as api\n\n# glove_vectors = api.load('glove-twitter-200')\n# print(glove_vectors.get_vector('tweeting'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SENTENCE_VECTOR_TYPE = 'mean'  # 'weighted'\nVECTOR_SIZE = 300\n\ndef mean_sentence_vector():\n    summarized_vector = np.zeros(VECTOR_SIZE)\n    sentences_vectors = []\n    \n    limit = len(train_tw)\n    for row in range(limit):\n        words_vectors_dict = train_tw.loc[row, 'FastText vectors of words']\n        for vector in words_vectors_dict.values():\n            summarized_vector += vector\n        number = len(words_vectors_dict.values())\n        sentence_vector = summarized_vector / number\n\n        sentences_vectors.append(sentence_vector)\n        \n    train_tw['sentence vector'] = sentences_vectors\n\ndef weighted_sentence_vector():\n    \n    corpus = train_tw.processed_text\n    vectorizer = TfidfVectorizer(token_pattern=r'(?!<\\w)([a-zA-Z]+)(?!\\w)')\n    tfidf_vectors = vectorizer.fit_transform(corpus)\n    # token_pattern=r'(?!<\\w)([a-zA-Z]+)(?!\\w)'\n\n    # print(len(vectorizer.get_feature_names_out()))\n    print(f'The array of Tf-idf words sentences has the shape: {tfidf_vectors.shape}.')\n    \n    vectors_df = pd.DataFrame(tfidf_vectors.toarray(),\n                          columns = vectorizer.get_feature_names_out()).T\n    print(vectors_df)\n    \n    limit = len(train_tw)\n    for row in range(limit):\n        words_list = train_tw['FastText vectors of words']\n        sentence_vector = np.zeros(VECTOR_SIZE)\n\n        for word in words_list.keys():\n            sentence_vector += words_list['word'] * vectors_df.loc[word, row]\n\n        train_tw['sentence_vector'] = sentence_vector\n\ndef vectorize_sentences():\n    if SENTENCE_VECTOR_TYPE == 'mean':\n        mean_sentence_vector()\n#         return train_tw['FastText vectors of words'].apply(mean_sentence_vector)\n    else:\n        return weighted_sentence_vector()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorize_sentences()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(train_tw.loc[19, 'sentence vector'].tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_tw['sentence vector'].tolist()\nY = train_tw.target\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42,\n                                                    shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(random_state=42)\nlr.fit(x_train, y_train)\ny_preds = lr.predict(x_val)\ny_preds_proba = lr.predict_proba(x_val)\nevaluate_results(y_val, y_preds, y_preds_proba)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_dataframe(filtered_cv_results):\n    \"\"\"Pretty print for filtered dataframe\"\"\"\n    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n        filtered_cv_results[\"mean_test_precision\"],\n        filtered_cv_results[\"std_test_precision\"],\n        filtered_cv_results[\"mean_test_recall\"],\n        filtered_cv_results[\"std_test_recall\"],\n        filtered_cv_results[\"params\"],\n    ):\n        print(\n            f\"precision: {mean_precision:0.3f} (±{std_precision:0.03f}),\"\n            f\" recall: {mean_recall:0.3f} (±{std_recall:0.03f}),\"\n            f\" for {params}\"\n        )\n    print()\n\n\ndef refit_estimator(cv_results):\n    \"\"\"Define the strategy to select the best estimator.\n\n    The strategy defined here is to filter-out all results below a precision threshold\n    of 0.98, rank the remaining by recall and keep all models with one standard\n    deviation of the best by recall. Once these models are selected, we can select the\n    fastest model to predict.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy (masked) ndarrays\n        CV results as returned by the `GridSearchCV`.\n\n    Returns\n    -------\n    best_index : int\n        The index of the best estimator as it appears in `cv_results`.\n    \"\"\"\n    # print the info about the grid-search for the different scores\n    precision_threshold = 0.98\n\n    cv_results_ = pd.DataFrame(cv_results)\n    print(\"All grid-search results:\")\n    print_dataframe(cv_results_)\n\n    # Filter-out all results below the threshold\n    high_precision_cv_results = cv_results_[\n        cv_results_[\"mean_test_precision\"] > precision_threshold\n    ]\n\n    print(f\"Models with a precision higher than {precision_threshold}:\")\n    print_dataframe(high_precision_cv_results)\n\n    high_precision_cv_results = high_precision_cv_results[\n        [\n            \"mean_score_time\",\n            \"mean_test_recall\",\n            \"std_test_recall\",\n            \"mean_test_precision\",\n            \"std_test_precision\",\n            \"rank_test_recall\",\n            \"rank_test_precision\",\n            \"params\",\n        ]\n    ]\n\n    # Select the most performant models in terms of recall\n    # (within 1 sigma from the best)\n    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n    best_recall_threshold = best_recall - best_recall_std\n\n    high_recall_cv_results = high_precision_cv_results[\n        high_precision_cv_results[\"mean_test_recall\"] > best_recall_threshold\n    ]\n    print(\n        \"Out of the previously selected high precision models, we keep all the\\n\"\n        \"the models within one standard deviation of the highest recall model:\"\n    )\n    print_dataframe(high_recall_cv_results)\n\n    # From the best candidates, select the fastest model to predict\n    fastest_top_recall_high_precision_index = high_recall_cv_results[\n        \"mean_score_time\"\n    ].idxmin()\n\n    print(\n        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n        \"selected subset of best models based on precision and recall.\\n\"\n        \"Its scoring time is:\\n\\n\"\n        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n    )\n\n    return fastest_top_recall_high_precision_index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_params_grid = [{\n        'penalty': ('l1', 'l2', 'elasticnet'),\n        'tol': (1e-3, 1e-4),\n        'C': (1.0, 10.0, 50.0, 100.0),\n        'solver': ('saga', 'lbfgs'),\n        'max_iter': (100, 250, 50)\n        },\n        {'penalty': 'l2',\n         'tol': (1e-3, 1e-4),\n         'C': (1.0, 10.0, 50.0, 100.0),\n         'solver': ('sag', 'lbfgs', 'newton-cg', 'newton-cholesky'),\n         'max_iter': (100, 250, 50)\n        }\n]\nlr = LogisticRegression(random_state=42)\nscores = [\"precision\", \"recall\"]\n\nclf = GridSearchCV(estimator=lr, param_grid=lr_params_grid, scoring=scores,\n                  refit=refit_estimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}